{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def conf_matrix(y, y_pred, title, labels):\n", "    fig, ax =plt.subplots(figsize=(7.5,7.5))\n", "    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Purples\", fmt='g', cbar=False, annot_kws={\"size\":30})\n", "    plt.title(title, fontsize=25)\n", "    ax.xaxis.set_ticklabels(labels, fontsize=16) \n", "    ax.yaxis.set_ticklabels(labels, fontsize=14.5)\n", "    ax.set_ylabel('Test', fontsize=25)\n", "    ax.set_xlabel('Predicted', fontsize=25)\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import re\n", "import string\n", "import emoji\n", "import nltk\n", "from nltk.stem import WordNetLemmatizer, PorterStemmer\n", "from nltk.corpus import stopwords\n", "import time\n", "import numpy as np\n", "import pandas as pd\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "# from collections import Counter\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n", "from sklearn.naive_bayes import MultinomialNB\n", "from sklearn.metrics import classification_report, confusion_matrix\n", "from sklearn import preprocessing\n", "# from sklearn.preprocessing import LabelEncoder\n", "from sklearn.model_selection import train_test_split\n", "from imblearn.over_sampling import RandomOverSampler\n", "from langdetect import detect, LangDetectException\n", "import contractions\n", "from nltk.tokenize import word_tokenize\n", "from nltk.stem import WordNetLemmatizer\n", "from sklearn.utils import resample\n", "from sklearn.metrics import confusion_matrix, classification_report\n", "from transformers import BertTokenizer, AdamW\n", "from transformers import BertModel\n", "from transformers import get_linear_schedule_with_warmup\n", "from imblearn.over_sampling import RandomOverSampler\n", "import torch\n", "import torch.nn as nn\n", "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nltk.download('punkt')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set seed for reproducibility"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import random\n", "seed_value = 2042\n", "random.seed(seed_value)\n", "np.random.seed(seed_value)\n", "torch.manual_seed(seed_value)\n", "torch.cuda.manual_seed_all(seed_value)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set style for plots"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.set_style(\"whitegrid\")\n", "sns.despine()\n", "plt.style.use(\"seaborn-whitegrid\")\n", "plt.rc(\"figure\", autolayout=True)\n", "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define stop words for text cleaning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["stop_words = set(stopwords.words('english'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initialize lemmatizer for text cleaning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lemmatizer = WordNetLemmatizer()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"cyberbullying_tweets.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.head()\n", "df.info()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df.rename(columns={'tweet_text': 'text', 'cyberbullying_type': 'sentiment'})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.duplicated().sum()\n", "df = df[~df.duplicated()] \n", "df.info()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.sentiment.value_counts()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Clean emojis from text"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def strip_emoji(text):\n", "    return emoji.demojize(text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove punctuations, stopwords, links, mentions and new line characters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def strip_all_entities(text):\n", "    text = re.sub(r'\\r|\\n', ' ', text.lower())  # Replace newline and carriage return with space, and convert to lowercase\n", "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)  # Remove links and mentions\n", "    text = re.sub(r'[^\\x00-\\x7f]', '', text)  # Remove non-ASCII characters\n", "    banned_list = string.punctuation\n", "    table = str.maketrans('', '', banned_list)\n", "    text = text.translate(table)\n", "    text = ' '.join(word for word in text.split() if word not in stop_words)\n", "    return text"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def clean_hashtags(tweet):\n", "    # Remove hashtags at the end of the sentence\n", "    new_tweet = re.sub(r'(\\s+#[\\w-]+)+\\s*$', '', tweet).strip()\n", "    \n", "    # Remove the # symbol from hashtags in the middle of the sentence\n", "    new_tweet = re.sub(r'#([\\w-]+)', r'\\1', new_tweet).strip()\n", "    \n", "    return new_tweet"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Filter special characters such as & and $ present in some words"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def filter_chars(text):\n", "    return ' '.join('' if ('$' in word) or ('&' in word) else word for word in text.split())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove multiple spaces"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def remove_mult_spaces(text):\n", "    return re.sub(r\"\\s\\s+\", \" \", text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Function to check if the text is in English, and return an empty string if it's not"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def filter_non_english(text):\n", "    try:\n", "        lang = detect(text)\n", "    except LangDetectException:\n", "        lang = \"unknown\"\n", "    return text if lang == \"en\" else \"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Expand contractions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def expand_contractions(text):\n", "    return contractions.fix(text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove numbers"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def remove_numbers(text):\n", "    return re.sub(r'\\d+', '', text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lemmatize words"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def lemmatize(text):\n", "    words = word_tokenize(text)\n", "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n", "    return ' '.join(lemmatized_words)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove short words"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def remove_short_words(text, min_len=2):\n", "    words = text.split()\n", "    long_words = [word for word in words if len(word) >= min_len]\n", "    return ' '.join(long_words)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Replace elongated words with their base form"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def replace_elongated_words(text):\n", "    regex_pattern = r'\\b(\\w+)((\\w)\\3{2,})(\\w*)\\b'\n", "    return re.sub(regex_pattern, r'\\1\\3\\4', text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove repeated punctuation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def remove_repeated_punctuation(text):\n", "    return re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove extra whitespace"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def remove_extra_whitespace(text):\n", "    return ' '.join(text.split())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def remove_url_shorteners(text):\n", "    return re.sub(r'(?:http[s]?://)?(?:www\\.)?(?:bit\\.ly|goo\\.gl|t\\.co|tinyurl\\.com|tr\\.im|is\\.gd|cli\\.gs|u\\.nu|url\\.ie|tiny\\.cc|alturl\\.com|ow\\.ly|bit\\.do|adoro\\.to)\\S+', '', text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove spaces at the beginning and end of the tweet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def remove_spaces_tweets(tweet):\n", "    return tweet.strip()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove short tweets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def remove_short_tweets(tweet, min_words=3):\n", "    words = tweet.split()\n", "    return tweet if len(words) >= min_words else \"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Function to call all the cleaning functions in the correct order"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def clean_tweet(tweet):\n", "    tweet = strip_emoji(tweet)\n", "    tweet = expand_contractions(tweet)\n", "    tweet = filter_non_english(tweet)\n", "    tweet = strip_all_entities(tweet)\n", "    tweet = clean_hashtags(tweet)\n", "    tweet = filter_chars(tweet)\n", "    tweet = remove_mult_spaces(tweet)\n", "    tweet = remove_numbers(tweet)\n", "    tweet = lemmatize(tweet)\n", "    tweet = remove_short_words(tweet)\n", "    tweet = replace_elongated_words(tweet)\n", "    tweet = remove_repeated_punctuation(tweet)\n", "    tweet = remove_extra_whitespace(tweet)\n", "    tweet = remove_url_shorteners(tweet)\n", "    tweet = remove_spaces_tweets(tweet)\n", "    tweet = remove_short_tweets(tweet)\n", "    tweet = ' '.join(tweet.split())  # Remove multiple spaces between words\n", "    return tweet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df['text_clean'] = [clean_tweet(tweet) for tweet in df['text']]\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f'There are around {int(df[\"text_clean\"].duplicated().sum())} duplicated tweets, we will remove them.')\n", "df.drop_duplicates(\"text_clean\", inplace=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.sentiment.value_counts()\n", "df = df[df[\"sentiment\"]!=\"other_cyberbullying\"]\n", "sentiments = [\"religion\",\"age\",\"ethnicity\",\"gender\",\"not bullying\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df['text_len'] = [len(text.split()) for text in df.text_clean]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(7,5))\n", "ax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\n", "plt.title('Count of tweets with less than 10 words', fontsize=20)\n", "plt.yticks([])\n", "ax.bar_label(ax.containers[0])\n", "plt.ylabel('count')\n", "plt.xlabel('')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.sort_values(by=['text_len'], ascending=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16,5))\n", "ax = sns.countplot(x='text_len', data=df[(df['text_len']<=1000) & (df['text_len']>10)], palette='Blues_r')\n", "plt.title('Count of tweets with high number of words', fontsize=25)\n", "plt.yticks([])\n", "ax.bar_label(ax.containers[0])\n", "plt.ylabel('count')\n", "plt.xlabel('')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df[df['text_len'] < df['text_len'].quantile(0.995)]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["max_len = np.max(df['text_len'])\n", "max_len "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.sort_values(by=[\"text_len\"], ascending=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df['sentiment'] = df['sentiment'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = df['text_clean']\n", "y = df['sentiment']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Train-test split"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["rain-validation split"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=seed_value)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["(unique, counts) = np.unique(y_train, return_counts=True)\n", "np.asarray((unique, counts)).T"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ros = RandomOverSampler()\n", "X_train, y_train = ros.fit_resample(np.array(X_train).reshape(-1, 1), np.array(y_train).reshape(-1, 1));\n", "train_os = pd.DataFrame(list(zip([x[0] for x in X_train], y_train)), columns = ['text_clean', 'sentiment']);"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train = train_os['text_clean'].values\n", "y_train = train_os['sentiment'].values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["(unique, counts) = np.unique(y_train, return_counts=True)\n", "np.asarray((unique, counts)).T"]}, {"cell_type": "markdown", "metadata": {}, "source": ["aives Bayes model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf = CountVectorizer()\n", "X_train_cv =  clf.fit_transform(X_train)\n", "X_test_cv = clf.transform(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\n", "X_train_tf = tf_transformer.transform(X_train_cv)\n", "X_test_tf = tf_transformer.transform(X_test_cv)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nb_clf = MultinomialNB()\n", "nb_clf.fit(X_train_tf, y_train)\n", "nb_pred = nb_clf.predict(X_test_tf)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('Classification Report for Naive Bayes:\\n',classification_report(y_test, nb_pred, target_names=sentiments))\n", "conf_matrix(y_test,nb_pred,'Naive Bayes Sentiment Analysis\\nConfusion Matrix', sentiments)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ERT classification"]}, {"cell_type": "markdown", "metadata": {}, "source": ["rain-validation-test split"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = df['text_clean'].values\n", "y = df['sentiment'].values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)\n", "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=seed_value)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ros = RandomOverSampler()\n", "X_train_os, y_train_os = ros.fit_resample(np.array(X_train).reshape(-1,1),np.array(y_train).reshape(-1,1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train_os = X_train_os.flatten()\n", "y_train_os = y_train_os.flatten()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["(unique, counts) = np.unique(y_train_os, return_counts=True)\n", "np.asarray((unique, counts)).T"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ERT tokenization"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def bert_tokenizer(data):\n", "    input_ids = []\n", "    attention_masks = []\n", "    for sent in data:\n", "        encoded_sent = tokenizer.encode_plus(\n", "            text=sent,\n", "            add_special_tokens=True,\n", "            max_length=MAX_LEN,             \n", "            pad_to_max_length=True,\n", "            return_attention_mask=True      \n", "            )\n", "        input_ids.append(encoded_sent.get('input_ids'))\n", "        attention_masks.append(encoded_sent.get('attention_mask'))\n\n", "    # Convert lists to tensors\n", "    input_ids = torch.tensor(input_ids)\n", "    attention_masks = torch.tensor(attention_masks)\n", "    return input_ids, attention_masks"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Tokenize train tweets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in X_train]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Find the longest tokenized tweet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["max_len = max([len(sent) for sent in encoded_tweets])\n", "print('Max length: ', max_len)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["MAX_LEN = 128\n", "train_inputs, train_masks = bert_tokenizer(X_train_os)\n", "val_inputs, val_masks = bert_tokenizer(X_valid)\n", "test_inputs, test_masks = bert_tokenizer(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Convert target columns to pytorch tensors format"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_labels = torch.from_numpy(y_train_os)\n", "val_labels = torch.from_numpy(y_valid)\n", "test_labels = torch.from_numpy(y_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["batch_size = 32"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create the DataLoader for our training set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_data = TensorDataset(train_inputs, train_masks, train_labels)\n", "train_sampler = RandomSampler(train_data)\n", "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create the DataLoader for our validation set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["val_data = TensorDataset(val_inputs, val_masks, val_labels)\n", "val_sampler = SequentialSampler(val_data)\n", "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create the DataLoader for our test set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["test_data = TensorDataset(test_inputs, test_masks, test_labels)\n", "test_sampler = SequentialSampler(test_data)\n", "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ERT modelling"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Bert_Classifier(nn.Module):\n", "    def __init__(self, freeze_bert=False):\n", "        super(Bert_Classifier, self).__init__()\n", "        # Specify hidden size of BERT, hidden size of the classifier, and number of labels\n", "        n_input = 768\n", "        n_hidden = 50\n", "        n_output = 5\n\n", "        # Instantiate BERT model\n", "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n", "        # Instantiate the classifier (a fully connected layer followed by a ReLU activation and another fully connected layer)\n", "        self.classifier = nn.Sequential(\n", "            nn.Linear(n_input, n_hidden),\n", "            nn.ReLU(),\n", "            nn.Linear(n_hidden, n_output)\n", "        )\n\n", "        # Freeze the BERT model weights if freeze_bert is True (useful for feature extraction without fine-tuning)\n", "        if freeze_bert:\n", "            for param in self.bert.parameters():\n", "                param.requires_grad = False\n", "    def forward(self, input_ids, attention_mask):\n", "        # Feed input data (input_ids and attention_mask) to BERT\n", "        outputs = self.bert(input_ids=input_ids,\n", "                            attention_mask=attention_mask)\n\n", "        # Extract the last hidden state of the `[CLS]` token from the BERT output (useful for classification tasks)\n", "        last_hidden_state_cls = outputs[0][:, 0, :]\n\n", "        # Feed the extracted hidden state to the classifier to compute logits\n", "        logits = self.classifier(last_hidden_state_cls)\n", "        return logits\n", "    \n", "# Function for initializing the BERT Classifier model, optimizer, and learning rate scheduler\n", "def initialize_model(epochs=4):\n", "    # Instantiate Bert Classifier\n", "    bert_classifier = Bert_Classifier(freeze_bert=False)\n", "    bert_classifier.to(device)\n\n", "    # Set up optimizer\n", "    optimizer = AdamW(bert_classifier.parameters(),\n", "                      lr=5e-5,    # learning rate, set to default value\n", "                      eps=1e-8    # decay, set to default value\n", "                      )\n\n", "    # Calculate total number of training steps\n", "    total_steps = len(train_dataloader) * epochs\n\n", "    # Define the learning rate scheduler\n", "    scheduler = get_linear_schedule_with_warmup(optimizer,\n", "                                                num_warmup_steps=0, # Default value\n", "                                                num_training_steps=total_steps)\n", "    return bert_classifier, optimizer, scheduler"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "EPOCHS=2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bert_classifier, optimizer, scheduler = initialize_model(epochs=EPOCHS)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ERT training<br>\n", "Define Cross entropy Loss function for the multiclass classification task"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loss_fn = nn.CrossEntropyLoss()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def bert_train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n", "    print(\"Start training...\\n\")\n", "    for epoch_i in range(epochs):\n", "        print(\"-\"*10)\n", "        print(\"Epoch : {}\".format(epoch_i+1))\n", "        print(\"-\"*10)\n", "        print(\"-\"*38)\n", "        print(f\"{'BATCH NO.':^7} | {'TRAIN LOSS':^12} | {'ELAPSED (s)':^9}\")\n", "        print(\"-\"*38)\n\n", "        # Measure the elapsed time of each epoch\n", "        t0_epoch, t0_batch = time.time(), time.time()\n\n", "        # Reset tracking variables at the beginning of each epoch\n", "        total_loss, batch_loss, batch_counts = 0, 0, 0\n", "        \n", "        ###TRAINING###\n\n", "        # Put the model into the training mode\n", "        model.train()\n", "        for step, batch in enumerate(train_dataloader):\n", "            batch_counts +=1\n", "            \n", "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n", "            # Zero out any previously calculated gradients\n", "            model.zero_grad()\n\n", "            # Perform a forward pass and get logits.\n", "            logits = model(b_input_ids, b_attn_mask)\n\n", "            # Compute loss and accumulate the loss values\n", "            loss = loss_fn(logits, b_labels)\n", "            batch_loss += loss.item()\n", "            total_loss += loss.item()\n\n", "            # Perform a backward pass to calculate gradients\n", "            loss.backward()\n\n", "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n", "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n", "            # Update model parameters:\n", "            # fine tune BERT params and train additional dense layers\n", "            optimizer.step()\n", "            # update learning rate\n", "            scheduler.step()\n\n", "            # Print the loss values and time elapsed for every 100 batches\n", "            if (step % 100 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n", "                # Calculate time elapsed for 20 batches\n", "                time_elapsed = time.time() - t0_batch\n", "                \n", "                print(f\"{step:^9} | {batch_loss / batch_counts:^12.6f} | {time_elapsed:^9.2f}\")\n", "                # Reset batch tracking variables\n", "                batch_loss, batch_counts = 0, 0\n", "                t0_batch = time.time()\n\n", "        # Calculate the average loss over the entire training data\n", "        avg_train_loss = total_loss / len(train_dataloader)\n\n", "        ###EVALUATION###\n", "        \n", "        # Put the model into the evaluation mode\n", "        model.eval()\n", "        \n", "        # Define empty lists to host accuracy and validation for each batch\n", "        val_accuracy = []\n", "        val_loss = []\n", "        for batch in val_dataloader:\n", "            batch_input_ids, batch_attention_mask, batch_labels = tuple(t.to(device) for t in batch)\n", "            \n", "            # We do not want to update the params during the evaluation,\n", "            # So we specify that we dont want to compute the gradients of the tensors\n", "            # by calling the torch.no_grad() method\n", "            with torch.no_grad():\n", "                logits = model(batch_input_ids, batch_attention_mask)\n", "            loss = loss_fn(logits, batch_labels)\n", "            val_loss.append(loss.item())\n\n", "            # Get the predictions starting from the logits (get index of highest logit)\n", "            preds = torch.argmax(logits, dim=1).flatten()\n\n", "            # Calculate the validation accuracy \n", "            accuracy = (preds == batch_labels).cpu().numpy().mean() * 100\n", "            val_accuracy.append(accuracy)\n\n", "        # Compute the average accuracy and loss over the validation set\n", "        val_loss = np.mean(val_loss)\n", "        val_accuracy = np.mean(val_accuracy)\n", "        \n", "        # Print performance over the entire training data\n", "        time_elapsed = time.time() - t0_epoch\n", "        print(\"-\"*61)\n", "        print(f\"{'AVG TRAIN LOSS':^12} | {'VAL LOSS':^10} | {'VAL ACCURACY (%)':^9} | {'ELAPSED (s)':^9}\")\n", "        print(\"-\"*61)\n", "        print(f\"{avg_train_loss:^14.6f} | {val_loss:^10.6f} | {val_accuracy:^17.2f} | {time_elapsed:^9.2f}\")\n", "        print(\"-\"*61)\n", "        print(\"\\n\")\n", "    \n", "    print(\"Training complete!\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bert_train(bert_classifier, train_dataloader, val_dataloader, epochs=EPOCHS)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def bert_predict(model, test_dataloader):\n", "    \n", "    # Define empty list to host the predictions\n", "    preds_list = []\n", "    \n", "    # Put the model into evaluation mode\n", "    model.eval()\n", "    \n", "    for batch in test_dataloader:\n", "        batch_input_ids, batch_attention_mask = tuple(t.to(device) for t in batch)[:2]\n", "        \n", "        # Avoid gradient calculation of tensors by using \"no_grad()\" method\n", "        with torch.no_grad():\n", "            logit = model(batch_input_ids, batch_attention_mask)\n", "        \n", "        # Get index of highest logit\n", "        pred = torch.argmax(logit,dim=1).cpu().numpy()\n", "        # Append predicted class to list\n", "        preds_list.extend(pred)\n", "    return preds_list"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bert_preds = bert_predict(bert_classifier, test_dataloader)\n", "print('Classification Report for BERT :\\n', classification_report(y_test, bert_preds, target_names=sentiments))\n", "conf_matrix(y_test, bert_preds,' BERT Sentiment Analysis\\nConfusion Matrix', sentiments)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}